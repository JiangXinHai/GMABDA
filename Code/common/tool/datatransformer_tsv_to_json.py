import csv
import json
import spacy
import re
from string import punctuation


def clean_word(word):
    """æ¸…ç†å•è¯ä¸­çš„æ ‡ç‚¹ç¬¦å·å’Œç©ºç™½å­—ç¬¦"""
    if word.strip() == "$T$":
        return "$T$"
    return word.strip(punctuation + ' \t\n\r').strip()


def add_spaces_around_special_tokens(text):
    """ä¸º$T$å’Œæ ‡ç‚¹å·¦å³æ·»åŠ ç©ºæ ¼ï¼Œç¡®ä¿ç‹¬ç«‹æˆè¯"""
    text = re.sub(r'\s*\$T\$\s*', ' $T$ ', text)
    punc_to_process = r',\. : ; ! \? @ # % & \* \( \) \[ \] \{ \} < > /'
    text = re.sub(r'(\s*)([' + punc_to_process + r'])(\s*)', r' \2 ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def get_word_index_in_list(word, word_list, case_sensitive=False):
    """æŸ¥æ‰¾å•è¯åœ¨åˆ—è¡¨ä¸­çš„ä½ç½®"""
    for idx, w in enumerate(word_list):
        clean_w = clean_word(w)
        clean_target = clean_word(word)
        if case_sensitive:
            if clean_w == clean_target or w == word:
                return idx
        else:
            if clean_w.lower() == clean_target.lower() or w.lower() == word.lower():
                return idx
    return -1


def extract_pure_nouns(processed_text, word_list, nlp):
    """
    ä»…æå–çº¯åè¯ï¼Œè¿‡æ»¤æ‰€æœ‰ä¿®é¥°æˆåˆ†ï¼š
    - ä¿ç•™ï¼šNOUNï¼ˆæ™®é€šåè¯ï¼‰ã€PROPNï¼ˆä¸“æœ‰åè¯ï¼‰
    - è¿‡æ»¤ï¼šé™å®šè¯ï¼ˆthe, a, anï¼‰ã€å‰¯è¯ã€å½¢å®¹è¯ã€ä»£è¯ç­‰
    """
    doc = nlp(processed_text)
    pure_nouns = []
    used_indices = set()
    
    # éœ€è¦æ’é™¤çš„å¸¸è§éåè¯è¯
    excluded_words = {'the', 'a', 'an', 'this', 'that', 'these', 'those', 
                      'my', 'your', 'his', 'her', 'its', 'our', 'their',
                      'just', 'only', 'very', 'more', 'most'}

    # å¤„ç†åè¯çŸ­è¯­ï¼Œä»…ä¿ç•™å…¶ä¸­çš„çº¯åè¯
    for chunk in doc.noun_chunks:
        chunk_pure_nouns = []
        chunk_indices = []
        
        for token in chunk:
            # ä»…ä¿ç•™åè¯è¯æ€§ï¼Œä¸”ä¸åœ¨æ’é™¤åˆ—è¡¨ä¸­
            if (token.pos_ in ['NOUN', 'PROPN'] and 
                clean_word(token.text).lower() not in excluded_words):
                
                idx = get_word_index_in_list(token.text, word_list)
                if idx != -1 and idx not in used_indices:
                    chunk_pure_nouns.append(word_list[idx])
                    chunk_indices.append(idx)
        
        # åªæ·»åŠ åŒ…å«åè¯çš„çŸ­è¯­
        if chunk_pure_nouns:
            pure_nouns.append(chunk_pure_nouns)
            for idx in chunk_indices:
                used_indices.add(idx)

    # è¡¥å……é—æ¼çš„å•ä¸ªåè¯
    for idx, word in enumerate(word_list):
        if idx in used_indices:
            continue
            
        clean_w = clean_word(word)
        if len(clean_w) < 2:
            continue
            
        # æ’é™¤å¸¸è§éåè¯è¯
        if clean_w.lower() in excluded_words:
            continue
            
        # æ£€æŸ¥æ˜¯å¦ä¸ºåè¯
        doc_single = nlp(word)
        for token in doc_single:
            if token.pos_ in ['NOUN', 'PROPN']:
                pure_nouns.append([word])
                used_indices.add(idx)
                break

    return pure_nouns


def convert_tsv_to_json(tsv_file, json_file, polarity_map=None):
    """TSVè½¬JSONè½¬æ¢å™¨ï¼ˆèšåˆç›¸åŒimage_idï¼Œä»…ä¿ç•™çº¯åè¯ï¼‰"""
    if polarity_map is None:
        polarity_map = {
            '0': 'NEU',
            '1': 'POS',
            '-1': 'NEG'
        }

    try:
        nlp = spacy.load("en_core_web_sm")
        print("âœ… æˆåŠŸåŠ è½½spaCyæ¨¡å‹: en_core_web_sm")
    except OSError:
        print("âŒ è¯·å…ˆå®‰è£…spaCyæ¨¡å‹ï¼špython -m spacy download en_core_web_sm")
        return

    # -------------------------- å…³é”®ä¿®æ”¹1ï¼šç”¨å­—å…¸æŒ‰image_idèšåˆæ•°æ® --------------------------
    # å­—å…¸ç»“æ„ï¼škey=image_idï¼Œvalue=è¯¥image_idå¯¹åº”çš„å®Œæ•´æ•°æ®ï¼ˆå«wordsã€aspectsã€nounç­‰ï¼‰
    image_data_dict = {}

    with open(tsv_file, 'r', encoding='utf-8') as tsvf:
        tsv_reader = csv.reader(tsvf, delimiter='\t')
        headers = next(tsv_reader)  # è·³è¿‡è¡¨å¤´ï¼ˆindex	#1 Label	#2 ImageID	#3 String	#3 Stringï¼‰
        
        for row_num, row in enumerate(tsv_reader, 1):
            if len(row) < 5:
                print(f"âš ï¸  è¡Œ{row_num}ï¼šå­—æ®µä¸è¶³ï¼ˆéœ€5åˆ—ï¼‰ï¼Œè·³è¿‡")
                continue

            try:
                # è§£æTSVè¡Œæ•°æ®ï¼ˆå¯¹åº”è¡¨å¤´é¡ºåºï¼‰
                index = row[0]          # åºå·
                label = row[1]          # ææ€§æ ‡ç­¾ï¼ˆå¦‚1ã€2ã€0ï¼‰
                image_id = row[2]       # æ ¸å¿ƒèšåˆé”®ï¼šimage_id
                tweet_text = row[3].strip()  # åŸå§‹æ–‡æœ¬ï¼ˆå«$T$ï¼‰
                term_str = row[4].strip()    # $T$å¯¹åº”çš„æ›¿æ¢æœ¯è¯­ï¼ˆå¦‚Tygaã€Cooperstownï¼‰
            except Exception as e:
                print(f"âš ï¸  è¡Œ{row_num}ï¼šè§£æå¤±è´¥ï¼ˆ{str(e)}ï¼‰ï¼Œè·³è¿‡")
                continue

            # 1. å¤„ç†æ–‡æœ¬æ ¼å¼ï¼ˆæ›¿æ¢$T$ã€åˆ†å‰²å•è¯ï¼‰
            text_with_spaces = add_spaces_around_special_tokens(tweet_text)
            processed_text = text_with_spaces.replace('$T$', term_str)  # ç”¨æœ¯è¯­æ›¿æ¢$T$
            words = [w.strip() for w in processed_text.split() if w.strip()]  # åˆ†å‰²ä¸ºå•è¯åˆ—è¡¨
            if not words:
                print(f"âš ï¸  è¡Œ{row_num}ï¼šå¤„ç†åæ–‡æœ¬ä¸ºç©ºï¼Œè·³è¿‡")
                continue

            # 2. è®¡ç®—å½“å‰æœ¯è¯­ï¼ˆterm_strï¼‰åœ¨wordsä¸­çš„ä½ç½®ï¼ˆfrom/toï¼‰
            term_words = [w.strip() for w in term_str.split() if w.strip()]  # æœ¯è¯­åˆ†å‰²ä¸ºå•è¯
            aspect_from, aspect_to = 0, 1  # é»˜è®¤ä½ç½®ï¼ˆè‹¥æœ¯è¯­ä¸ºç©ºåˆ™ç”¨é»˜è®¤ï¼‰
            if term_words:
                aspect_from = get_word_index_in_list(term_words[0], words)  # æœ¯è¯­èµ·å§‹ä½ç½®
                if aspect_from != -1:
                    aspect_to = aspect_from + len(term_words)  # æœ¯è¯­ç»“æŸä½ç½®ï¼ˆå·¦é—­å³å¼€ï¼‰

            # 3. ç”Ÿæˆå½“å‰è¡Œçš„aspectï¼ˆæœ¯è¯­ææ€§ä¿¡æ¯ï¼‰
            current_aspect = {
                "from": aspect_from,
                "to": aspect_to,
                "polarity": polarity_map.get(label, 'NEU'),  # æ˜ å°„ææ€§ï¼ˆå¦‚1â†’NEUï¼‰
                "term": term_words  # æœ¯è¯­çš„å•è¯åˆ—è¡¨
            }

            # 4. æå–å½“å‰æ–‡æœ¬çš„çº¯åè¯ï¼ˆæ¯ä¸ªimage_idåªéœ€æå–ä¸€æ¬¡ï¼Œä¸æ–‡æœ¬å¯¹åº”ï¼‰
            current_nouns = extract_pure_nouns(processed_text, words, nlp)

            # -------------------------- å…³é”®ä¿®æ”¹2ï¼šæŒ‰image_idèšåˆé€»è¾‘ --------------------------
            if image_id not in image_data_dict:
                # è‹¥image_idé¦–æ¬¡å‡ºç°ï¼šåˆå§‹åŒ–è¯¥image_idçš„æ•°æ®ç»“æ„
                image_data_dict[image_id] = {
                    "words": words,                  # æ–‡æœ¬å¯¹åº”çš„å•è¯åˆ—è¡¨ï¼ˆåŒimage_idæ–‡æœ¬åº”ä¸€è‡´ï¼Œè‹¥ä¸ä¸€è‡´å–é¦–æ¬¡ï¼‰
                    "image_id": image_id,            # èšåˆé”®
                    "aspects": [current_aspect],     # åˆå§‹åŒ–aspectsåˆ—è¡¨ï¼ŒåŠ å…¥å½“å‰aspect
                    "opinions": [{"term": []}],      # å›ºå®šç»“æ„ï¼ˆåŒåŸä»£ç ï¼‰
                    "noun": current_nouns            # çº¯åè¯åˆ—è¡¨ï¼ˆåŒimage_idæ–‡æœ¬å¯¹åº”ï¼Œå–é¦–æ¬¡ï¼‰
                }
            else:
                # è‹¥image_idå·²å­˜åœ¨ï¼šä»…è¿½åŠ aspectåˆ°aspectsåˆ—è¡¨ï¼ˆä¸é‡å¤ç”Ÿæˆwordså’Œnounï¼‰
                image_data_dict[image_id]["aspects"].append(current_aspect)

            # è¿›åº¦æç¤ºï¼ˆæ¯100è¡Œæ‰“å°ä¸€æ¬¡ï¼‰
            if row_num % 100 == 0:
                print(f"ğŸš€ å·²å¤„ç†{row_num}è¡Œï¼Œå½“å‰èšåˆçš„image_idæ•°é‡ï¼š{len(image_data_dict)}")

    # -------------------------- å…³é”®ä¿®æ”¹3ï¼šå­—å…¸è½¬åˆ—è¡¨ï¼ˆæœ€ç»ˆJSONæ ¼å¼ï¼‰ --------------------------
    # å°†image_data_dictçš„valuesè½¬æ¢ä¸ºåˆ—è¡¨ï¼ˆJSONæ•°ç»„æ ¼å¼ï¼‰
    result = list(image_data_dict.values())

    # å†™å…¥è¾“å‡ºJSONæ–‡ä»¶
    with open(json_file, 'w', encoding='utf-8') as jsonf:
        json.dump(result, jsonf, indent=4, ensure_ascii=False)

    print(f"\nğŸ‰ è½¬æ¢å®Œæˆï¼å…±å¤„ç† {len(result)} ä¸ªå”¯ä¸€image_idï¼ˆåŸå§‹TSVè¡Œæ•°ï¼š{row_num}ï¼‰")
    print(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶ï¼š{json_file}")
    if result:
        print(f"ğŸ” ç¤ºä¾‹ç»“æœï¼ˆé¦–ä¸ªimage_idï¼‰ï¼š")
        print(f"  - image_id: {result[0]['image_id']}")
        print(f"  - aspectsæ•°é‡: {len(result[0]['aspects'])}")
        print(f"  - çº¯åè¯: {result[0]['noun'][:3]}")  # ä»…æ˜¾ç¤ºå‰3ä¸ªåè¯


if __name__ == "__main__":
    # 1. é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆæ ¹æ®å®é™…è·¯å¾„è°ƒæ•´ï¼‰
    INPUT_TSV_train = "/home/jiangxinhai/GMABDA/Data/twitter2015/generator_texts/train_texts/paraphrased_tweets_2025-10-02.tsv"
    OUTPUT_JSON_train = "/home/jiangxinhai/GMABDA/Data/twitter2015/generator_texts/train_texts/train_generated.json"

    # 2. ææ€§æ˜ å°„é…ç½®ï¼ˆä¸TSVçš„#1 Labelå¯¹åº”ï¼š0â†’NEGï¼Œ1â†’NEUï¼Œ2â†’POSï¼‰
    POLARITY_MAP = {'0': 'NEG', '1': 'NEU', '2': 'POS'}

    # 3. æµ‹è¯•ç¤ºä¾‹æ–‡æœ¬å¤„ç†æ•ˆæœï¼ˆéªŒè¯åè¯æå–é€»è¾‘ï¼‰
    test_text = "Tyga was seen changing his sexual orientation from pedophilia to messing with cougars in just a week ."
    print(f"ğŸ” æµ‹è¯•æ–‡æœ¬å¤„ç†æ•ˆæœï¼š")
    print(f"åŸå§‹æ–‡æœ¬: {test_text}")
    
    nlp_test = spacy.load("en_core_web_sm")
    test_words = test_text.split()
    test_nouns = extract_pure_nouns(test_text, test_words, nlp_test)
    print(f"æå–çš„çº¯åè¯: {test_nouns}")  # é¢„æœŸè¾“å‡º: [["Tyga"], ["orientation"], ["pedophilia"], ["cougars"], ["week"]]
    print("-" * 50)

    # 4. å¯åŠ¨TSVè½¬JSON
    print(f"ğŸ“Œ å¼€å§‹å¤„ç†è®­ç»ƒé›†TSVï¼š{INPUT_TSV_train}")
    convert_tsv_to_json(INPUT_TSV_train, OUTPUT_JSON_train, POLARITY_MAP)